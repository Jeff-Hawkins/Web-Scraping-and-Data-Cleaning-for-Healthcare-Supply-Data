{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb8d622-6a60-4d7c-a430-e6f5f2619c4f",
   "metadata": {},
   "source": [
    "# Combining and scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42005d48-bd77-4c47-bcde-699551f3c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load the provided Excel file with the combined dataset\n",
    "input_file = '/path/to/Combined_Data.xlsx'\n",
    "\n",
    "# Load the combined data\n",
    "data = pd.read_excel(input_file)\n",
    "\n",
    "# Standardize column names by stripping leading/trailing spaces\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Dynamically determine the correct column name for item numbers\n",
    "if 'Item #' in data.columns:\n",
    "    item_column = 'Item #'\n",
    "elif 'Item No.' in data.columns:\n",
    "    item_column = 'Item No.'\n",
    "else:\n",
    "    raise KeyError(\"No valid item number column found in the data.\")\n",
    "\n",
    "# Dynamically detect the manufacturer item column name\n",
    "if 'Mfr #' in data.columns:\n",
    "    mfr_item_column = 'Mfr #'\n",
    "elif 'Mfr item #' in data.columns:\n",
    "    mfr_item_column = 'Mfr item #'\n",
    "else:\n",
    "    raise KeyError(\"No valid manufacturer item number column found in the data.\")\n",
    "\n",
    "# Convert item numbers to strings and handle invalid entries\n",
    "data[item_column] = data[item_column].apply(\n",
    "    lambda x: str(int(x)) if pd.notna(x) and str(x).replace(\".0\", \"\").isdigit() else None\n",
    ")\n",
    "\n",
    "# Drop rows with missing or invalid item numbers\n",
    "data = data.dropna(subset=[item_column])\n",
    "\n",
    "# Log the number of rows remaining after cleaning\n",
    "print(f\"Rows remaining after cleaning invalid item numbers: {len(data)}\")\n",
    "\n",
    "# Save cleaned data for verification\n",
    "cleaned_file = '/path/to/Cleaned_Data.xlsx'\n",
    "data.to_excel(cleaned_file, index=False)\n",
    "print(f\"Cleaned data saved to {cleaned_file}\")\n",
    "\n",
    "# Function to generate McKesson URL for a given item number\n",
    "def generate_mckesson_url(item_no):\n",
    "    return f\"https://mms.mckesson.com/product/{item_no}?src=CS\"\n",
    "\n",
    "# Updated scraping function using requests and BeautifulSoup\n",
    "def scrape_mckesson_data(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the first table on the page\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            print(f\"Table not found: {url}\")\n",
    "            return {'Mfr Name': None, 'Mfr item #': None}\n",
    "\n",
    "        # Extract data from the table\n",
    "        mfr_name = None\n",
    "        mfr_item_no = None\n",
    "\n",
    "        for row in table.find_all('tr'):\n",
    "            header = row.find('th').text.strip()\n",
    "            value = row.find('td').text.strip() if row.find('td') else None\n",
    "            if header == \"Manufacturer\":\n",
    "                mfr_name = value\n",
    "            elif header == \"Manufacturer #\":\n",
    "                mfr_item_no = value\n",
    "\n",
    "        # Log the scraped result\n",
    "        print(f\"Scraped: {url} -> Mfr Name: {mfr_name}, Mfr item #: {mfr_item_no}\")\n",
    "        return {'Mfr Name': mfr_name, 'Mfr item #': mfr_item_no}\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"HTTP error for {url}: {req_err}\")\n",
    "        return {'Mfr Name': None, 'Mfr item #': None, 'Error': str(req_err)}\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error scraping {url}: {e}\")\n",
    "        return {'Mfr Name': None, 'Mfr item #': None, 'Error': str(e)}\n",
    "\n",
    "# Generate McKesson URLs for each row in your data\n",
    "data['McKesson URL'] = data[item_column].apply(generate_mckesson_url)\n",
    "\n",
    "# Filter rows where manufacturer data is missing\n",
    "missing_data_rows = data[data['Mfr Name'].isna() | data[mfr_item_column].isna()]\n",
    "\n",
    "# Log missing data rows\n",
    "print(f\"Rows with missing data: {len(missing_data_rows)}\")\n",
    "missing_data_rows.to_excel('/path/to/Missing_Data_Rows.xlsx', index=False)\n",
    "\n",
    "# Prepare for parallel processing\n",
    "batch_size = 50\n",
    "batches = [missing_data_rows['McKesson URL'][i:i + batch_size] for i in range(0, len(missing_data_rows), batch_size)]\n",
    "\n",
    "failed_urls = []  # Track URLs where scraping failed\n",
    "results = []\n",
    "\n",
    "# Use ThreadPoolExecutor to process batches in parallel\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    future_to_batch = {\n",
    "        executor.submit(process_batch, batch.tolist(), idx + 1): idx\n",
    "        for idx, batch in enumerate(batches)\n",
    "    }\n",
    "    for future in future_to_batch:\n",
    "        batch_num = future_to_batch[future]\n",
    "        try:\n",
    "            batch_results = future.result()\n",
    "            results.extend(batch_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_num}: {e}\")\n",
    "            failed_urls.extend(batches[batch_num])\n",
    "\n",
    "# Log failed URLs\n",
    "if failed_urls:\n",
    "    with open('/path/to/Failed_URLs.txt', 'w') as f:\n",
    "        for url in failed_urls:\n",
    "            f.write(url + '\\n')\n",
    "    print(\"Failed URLs logged to Failed_URLs.txt\")\n",
    "\n",
    "# Update original data with results\n",
    "for result in results:\n",
    "    url = result.get('url')\n",
    "    mfr_name = result.get('Mfr Name')\n",
    "    mfr_item_no = result.get('Mfr item #')\n",
    "\n",
    "    # Match rows by URL and update data\n",
    "    data.loc[data['McKesson URL'] == url, 'Mfr Name'] = mfr_name\n",
    "    data.loc[data['McKesson URL'] == url, mfr_item_column] = mfr_item_no\n",
    "\n",
    "# Save updated dataset\n",
    "output_file = '/path/to/Updated_Mfg_Name_Search.xlsx'\n",
    "data.to_excel(output_file, index=False)\n",
    "print(f\"Updated data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf1784-adf9-4e4a-ad39-a79b044f01b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
